{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7296f836-0d09-4720-a006-e5d5ae18f8ac",
   "metadata": {},
   "source": [
    "# Gemini pass/maj-at-k experiments\n",
    "\n",
    "Run Gemini 1.5 Pro v2 with temperature 1 and top_p 0.95, sampling 64 times. Used zero-shot prompt (short answer) and got 8 completions at seeds 0-7 (as each call supports at most 8 completions). These samples are then used to compute pass@k and maj@k results for powers of 2 from 1 to 64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "370fd2c5-bb7f-4833-98b9-81720ec41902",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "BASEDIR = Path(\"/workspaces/HARP/\") / \"src\"  # Replace with your own basedir path for the repo\n",
    "\n",
    "sys.path.insert(0, str(BASEDIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4a7c672-a591-4098-9e86-6553ea110a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import copy\n",
    "import itertools\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "import pprint\n",
    "import re\n",
    "import textwrap\n",
    "import time\n",
    "import traceback\n",
    "from collections import Counter, defaultdict\n",
    "from typing import Any\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "from IPython.display import Markdown, clear_output, display\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import vertexai\n",
    "from vertexai.batch_prediction._batch_prediction import BatchPredictionJob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea512c09-e8dc-4760-ae95-77c76d21e181",
   "metadata": {},
   "outputs": [],
   "source": [
    "from eval.api import safe_unified_api_call\n",
    "from eval.costs import count_tokens, get_pricing\n",
    "from eval.eval import run_one, create_batch, make_answer_check_dict_from_jsonl\n",
    "from eval.parsing_lib import *\n",
    "from eval.latex_answer_check import *\n",
    "from eval.prompt import create_prompt\n",
    "from eval.prompts import *\n",
    "from eval.utils import read_jsonl, write_jsonl, get_uid, upload_blob, download_blob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1dd4fe-9bed-4b04-94cf-dd1aa8ccc05f",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38903393-4ea1-46a9-9bab-d424c335abf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4780"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = read_jsonl(BASEDIR / \"data/processed/HARP.jsonl\")\n",
    "dataset_map = {get_uid(p): p for p in dataset}\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532bd49a-7e93-4fc8-b508-3edae5528815",
   "metadata": {},
   "source": [
    "# Run eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a4a7847-7bd4-41d5-b73b-c3ec92f14d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertexai.init(project=os.environ.get(\"VERTEXAI_PROJECT_ID\"), location=\"us-central1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19b78a16-daec-4a9d-9e7d-49167f8d3602",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = os.environ.get(\"GCLOUD_BUCKET_NAME\")  # Should have the form \"cloud-ai-platform-<YOUR_BUCKET>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e28ac58-4efa-43ed-a7a9-2a3d317e5537",
   "metadata": {},
   "source": [
    "## Create batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0ea97c73-494b-4c11-be0f-552b7ff316e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want a total of 64, but the max number of completions in one request is 8\n",
    "# So let's make 8 runs, with incrementing values for the seed\n",
    "for seed in range(8):\n",
    "    batch = create_batch(\n",
    "        dataset,\n",
    "        api=\"google\",\n",
    "        model=\"gemini-1.5-pro-002\",\n",
    "        fewshot_messages=[],\n",
    "        system_prompt=gemini_0shot_sysprompt,\n",
    "        max_tokens=2048,\n",
    "        num_completions=8,\n",
    "        temperature=1,\n",
    "        top_p=0.95,\n",
    "        seed=seed,\n",
    "        stop_sequences=[\"I hope it is correct.\"],\n",
    "        # just to remove irrelevant params\n",
    "        logprobs=None,\n",
    "    )\n",
    "    write_jsonl(batch, BASEDIR / f\"inputs/short_answer/gemini-1.5-pro-002/batch_passk_seed{seed}.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d29b20-3d43-4c76-af1e-66d60e975b8e",
   "metadata": {},
   "source": [
    "## Upload to cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c444c58f-4c56-4467-9130-0d4b4643cb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for seed in range(8):\n",
    "    upload_blob(\n",
    "        BUCKET_NAME,\n",
    "        BASEDIR / f\"inputs/short_answer/gemini-1.5-pro-002/batch_passk_seed{seed}.jsonl\",\n",
    "        f\"prompt_data/short_answer/gemini-1.5-pro-002/batch_passk_seed{seed}.jsonl\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0ac3e5-3ff5-431f-9134-802e3904a034",
   "metadata": {},
   "source": [
    "## Run batch job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb83032-17f7-4106-a862-74bd81efe22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for seed in range(8):\n",
    "    BatchPredictionJob.submit(\n",
    "        source_model=\"gemini-1.5-pro-002\",\n",
    "        input_dataset=f\"gs://{BUCKET_NAME}/prompt_data/short_answer/gemini-1.5-pro-002/batch_passk_seed{seed}.jsonl\",\n",
    "        output_uri_prefix=f\"gs://{BUCKET_NAME}/outputs/short_answer/gemini-1.5-pro-002/batch_passk_seed{seed}\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61519850-25f7-4b82-9f24-e8cce39cdaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_blob(\n",
    "    BUCKET_NAME,\n",
    "    \"outputs/short_answer/gemini-1.5-pro-002/batch_passk_seed0/prediction-model-<TIMESTAMP>/predictions.jsonl\",\n",
    "    BASEDIR / \"outputs/short_answer/gemini-1.5-pro-002/outputs_passk_seed0.jsonl\",\n",
    ")\n",
    "download_blob(\n",
    "    BUCKET_NAME,\n",
    "    \"outputs/short_answer/gemini-1.5-pro-002/batch_passk_seed1/prediction-model-<TIMESTAMP>/predictions.jsonl\",\n",
    "    BASEDIR / \"outputs/short_answer/gemini-1.5-pro-002/outputs_passk_seed1.jsonl\",\n",
    ")\n",
    "download_blob(\n",
    "    BUCKET_NAME,\n",
    "    \"outputs/short_answer/gemini-1.5-pro-002/batch_passk_seed2/prediction-model-<TIMESTAMP>/predictions.jsonl\",\n",
    "    BASEDIR / \"outputs/short_answer/gemini-1.5-pro-002/outputs_passk_seed2.jsonl\",\n",
    ")\n",
    "download_blob(\n",
    "    BUCKET_NAME,\n",
    "    \"outputs/short_answer/gemini-1.5-pro-002/batch_passk_seed3/prediction-model-<TIMESTAMP>/predictions.jsonl\",\n",
    "    BASEDIR / \"outputs/short_answer/gemini-1.5-pro-002/outputs_passk_seed3.jsonl\",\n",
    ")\n",
    "download_blob(\n",
    "    BUCKET_NAME,\n",
    "    \"outputs/short_answer/gemini-1.5-pro-002/batch_passk_seed4/prediction-model-<TIMESTAMP>/predictions.jsonl\",\n",
    "    BASEDIR / \"outputs/short_answer/gemini-1.5-pro-002/outputs_passk_seed4.jsonl\",\n",
    ")\n",
    "download_blob(\n",
    "    BUCKET_NAME,\n",
    "    \"outputs/short_answer/gemini-1.5-pro-002/batch_passk_seed5/prediction-model-<TIMESTAMP>/predictions.jsonl\",\n",
    "    BASEDIR / \"outputs/short_answer/gemini-1.5-pro-002/outputs_passk_seed5.jsonl\",\n",
    ")\n",
    "download_blob(\n",
    "    BUCKET_NAME,\n",
    "    \"outputs/short_answer/gemini-1.5-pro-002/batch_passk_seed6/prediction-model-<TIMESTAMP>/predictions.jsonl\",\n",
    "    BASEDIR / \"outputs/short_answer/gemini-1.5-pro-002/outputs_passk_seed6.jsonl\",\n",
    ")\n",
    "download_blob(\n",
    "    BUCKET_NAME,\n",
    "    \"outputs/short_answer/gemini-1.5-pro-002/batch_passk_seed7/prediction-model-<TIMESTAMP>/predictions.jsonl\",\n",
    "    BASEDIR / \"outputs/short_answer/gemini-1.5-pro-002/outputs_passk_seed7.jsonl\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706d4e32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
